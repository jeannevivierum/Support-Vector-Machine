{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: Support Vector Machine\n",
        "subtitle: Jeanne Vivier\n",
        "format:\n",
        "  html:\n",
        "    code-fold: true\n",
        "---"
      ],
      "id": "e5bbf389"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| include: false\n",
        "import sys\n",
        "from pathlib import Path\n",
        "sys.path.append(str(Path(\"..\") / \"Code\"))\n",
        "from svm_source import *\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from sklearn import svm\n",
        "from sklearn import datasets\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.datasets import fetch_lfw_people\n",
        "from sklearn.decomposition import PCA\n",
        "from time import time\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "plt.style.use('ggplot')"
      ],
      "id": "e0d55291",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n",
        "Les machines à vecteurs de support (Support Vector Machine, SVM, en anglais), sont un ensemble de méthodes d'apprentissage supervisé utilisées pour la classification, la régression et la détection des valeurs aberrantes. La popularité des méthodes SVM, pour la classification binaire en particulier, provient du fait qu’elles reposent sur l’application d’algorithmes de recherche de règles de décision linéaires : on parle d’hyperplans (affines) séparateurs.\n",
        "\n",
        "## Question 1\n",
        "Pour commencer, écrivons un code qui va classifier la classe 1 contre la classe 2 du dataset $\\texttt{iris}$ en utilisant les deux premières variables et un noyau linéaire. \n"
      ],
      "id": "05ded380"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "X = scaler.fit_transform(X)\n",
        "y = iris.target\n",
        "X = X[y != 0, :2]\n",
        "y = y[y != 0]\n",
        "\n",
        "X, y = shuffle(X, y)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
        "\n",
        "parameters = {'kernel': ['linear'], 'C': list(np.logspace(-3, 3, 200))}\n",
        "\n",
        "clf_linear = GridSearchCV(SVC(),parameters,n_jobs=-1)\n",
        "\n",
        "clf_linear.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "print('Generalization score for linear kernel: %s, %s' %\n",
        "      (clf_linear.score(X_train, y_train),\n",
        "       clf_linear.score(X_test, y_test)))"
      ],
      "id": "ae18c5c6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ici, commenter.\n",
        "\n",
        "## Question 2\n",
        "\n",
        "Nous voulons alors comparer ce résultat avec un SVM basé sur noyau polynomial.\n"
      ],
      "id": "3cc4c638"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "Cs = list(np.logspace(-3, 3, 5))\n",
        "gammas = 10. ** np.arange(1, 2)\n",
        "degrees = np.r_[1, 2, 3]\n",
        "\n",
        "\n",
        "\n",
        "parameters = {'kernel': ['poly'], 'C': Cs, 'gamma': gammas, 'degree': degrees}\n",
        "\n",
        "\n",
        "clf_poly = GridSearchCV(SVC(),parameters,n_jobs=-1)\n",
        "\n",
        "clf_poly.fit(X_train,y_train)\n",
        "\n",
        "print(clf_poly.best_params_)\n",
        "\n",
        "print('Generalization score for polynomial kernel: %s, %s' %\n",
        "      (clf_poly.score(X_train, y_train),\n",
        "       clf_poly.score(X_test, y_test)))\n",
        "\n",
        "\n",
        "def f_linear(xx):\n",
        "    \"\"\"Classifier: needed to avoid warning due to shape issues\"\"\"\n",
        "    return clf_linear.predict(xx.reshape(1, -1))\n",
        "\n",
        "def f_poly(xx):\n",
        "    \"\"\"Classifier: needed to avoid warning due to shape issues\"\"\"\n",
        "    return clf_poly.predict(xx.reshape(1, -1))\n",
        "\n",
        "plt.ion()\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.subplot(131)\n",
        "plot_2d(X, y)\n",
        "plt.title(\"iris dataset\")\n",
        "\n",
        "plt.subplot(132)\n",
        "frontiere(f_linear, X, y)\n",
        "plt.title(\"linear kernel\")\n",
        "\n",
        "plt.subplot(133)\n",
        "frontiere(f_poly, X, y)\n",
        "\n",
        "plt.title(\"polynomial kernel\")\n",
        "plt.tight_layout()\n",
        "plt.draw()"
      ],
      "id": "efdc18db",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "surement faux ! comparer les résultats\n",
        "\n",
        "# SVM GUI\n",
        "\n",
        "## Question 3\n",
        "\n",
        "\n",
        "\n",
        "# Classification de visages\n",
        "\n",
        "Intéressons nous maintenant à un problème de classification de visages.  \n",
        "\n",
        "## Question 4\n",
        "\n",
        "Nous cherchons à montrer l'influence du paramètre de régularisation. Nous affichons dans la figure (METTRE REF) le score d'apprentissage en fonction de C sur une échelle logarithmique entre 1e5 et 1e-5(qui n'est rien d'autre que 1 - l'erreur de prédiction).\n"
      ],
      "id": "953a10e4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| include: false\n",
        "\n",
        "\"\"\"\n",
        "The dataset used in this example is a preprocessed excerpt\n",
        "of the \"Labeled Faces in the Wild\", aka LFW_:\n",
        "\n",
        "  http://vis-www.cs.umass.edu/lfw/lfw-funneled.tgz (233MB)\n",
        "\n",
        "  _LFW: http://vis-www.cs.umass.edu/lfw/\n",
        "\"\"\"\n",
        "\n",
        "lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4,\n",
        "                              color=True, funneled=False, slice_=None,\n",
        "                              download_if_missing=True)\n",
        "\n",
        "images = lfw_people.images\n",
        "n_samples, h, w, n_colors = images.shape\n",
        "\n",
        "target_names = lfw_people.target_names.tolist()\n",
        "\n",
        "names = ['Tony Blair', 'Colin Powell']\n",
        "\n",
        "\n",
        "idx0 = (lfw_people.target == target_names.index(names[0]))\n",
        "idx1 = (lfw_people.target == target_names.index(names[1]))\n",
        "images = np.r_[images[idx0], images[idx1]]\n",
        "n_samples = images.shape[0]\n",
        "y = np.r_[np.zeros(np.sum(idx0)), np.ones(np.sum(idx1))].astype(int)\n",
        "\n",
        "plot_gallery(images, np.arange(12))\n",
        "plt.show()\n",
        "\n",
        "X = (np.mean(images, axis=3)).reshape(n_samples, -1)\n",
        "\n",
        "\n",
        "X -= np.mean(X, axis=0)\n",
        "X /= np.std(X, axis=0)\n",
        "\n",
        "indices = np.random.permutation(X.shape[0])\n",
        "train_idx, test_idx = indices[:X.shape[0] // 2], indices[X.shape[0] // 2:]\n",
        "X_train, X_test = X[train_idx, :], X[test_idx, :]\n",
        "y_train, y_test = y[train_idx], y[test_idx]\n",
        "images_train, images_test = images[\n",
        "    train_idx, :, :, :], images[test_idx, :, :, :]"
      ],
      "id": "d093f9eb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"--- Linear kernel ---\")\n",
        "print(\"Fitting the classifier to the training set\")\n",
        "t0 = time()\n",
        "\n",
        "# fit a classifier (linear) and test all the Cs\n",
        "Cs = 10. ** np.arange(-5, 6)\n",
        "scores = []\n",
        "for C in Cs:\n",
        "\n",
        "    clf = SVC(kernel='linear', C=C)\n",
        "    clf.fit(X_train,y_train)\n",
        "\n",
        "    score = clf.score(X_train, y_train)\n",
        "    scores.append(score)\n",
        "\n",
        "ind = np.argmax(scores)\n",
        "print(\"Best C: {}\".format(Cs[ind]))\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(Cs, scores)\n",
        "plt.xlabel(\"Parametres de regularisation C\")\n",
        "plt.ylabel(\"Scores d'apprentissage\")\n",
        "plt.xscale(\"log\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "print(\"Best score: {}\".format(np.max(scores)))\n",
        "\n",
        "print(\"Predicting the people names on the testing set\")\n",
        "\n",
        "t0=time()\n",
        "clf= SVC(kernel='linear', C=Cs[ind])\n",
        "clf.fit(X_train,y_train)\n",
        "\n",
        "# fin du fait par moi, vérifier avec les autres\n",
        "\n",
        "print(\"done in %0.3fs\" % (time() - t0))\n",
        "# The chance level is the accuracy that will be reached when constantly predicting the majority class.\n",
        "print(\"Chance level : %s\" % max(np.mean(y), 1. - np.mean(y)))\n",
        "print(\"Accuracy : %s\" % clf.score(X_test, y_test))\n",
        "\n",
        "y_pred = y_pred[:y_test.shape[0]]\n",
        "prediction_titles = [title(y_pred[i], y_test[i], names)\n",
        "                     for i in range(y_pred.shape[0])]\n",
        "\n",
        "plot_gallery(images_test, prediction_titles)\n",
        "plt.show()\n",
        "\n",
        "####################################################################\n",
        "# Look at the coefficients\n",
        "plt.figure()\n",
        "plt.imshow(np.reshape(clf.coef_, (h, w)))\n",
        "plt.show()"
      ],
      "id": "ada9a225",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "COMMENTER\n",
        "\n",
        "## Question 5\n"
      ],
      "id": "68520803"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def run_svm_cv(_X, _y):\n",
        "    _indices = np.random.permutation(_X.shape[0])\n",
        "    _train_idx, _test_idx = _indices[:_X.shape[0] // 2], _indices[_X.shape[0] // 2:]\n",
        "    _X_train, _X_test = _X[_train_idx, :], _X[_test_idx, :]\n",
        "    _y_train, _y_test = _y[_train_idx], _y[_test_idx]\n",
        "\n",
        "    _parameters = {'kernel': ['linear'], 'C': list(np.logspace(-3, 3, 5))}\n",
        "    _svr = svm.SVC()\n",
        "    _clf_linear = GridSearchCV(_svr, _parameters)\n",
        "    _clf_linear.fit(_X_train, _y_train)\n",
        "\n",
        "    print('Generalization score for linear kernel: %s, %s \\n' %\n",
        "          (_clf_linear.score(_X_train, _y_train), _clf_linear.score(_X_test, _y_test)))\n",
        "\n",
        "print(\"Score sans variable de nuisance\")\n",
        "# fait par moi\n",
        "run_svm_cv(X, y)\n",
        "# fin du fait par moi\n",
        "\n",
        "print(\"Score avec variable de nuisance\")\n",
        "n_features = X.shape[1]\n",
        "# On rajoute des variables de nuisances\n",
        "sigma = 1\n",
        "noise = sigma * np.random.randn(n_samples, 300, )\n",
        "X_noisy = np.concatenate((X, noise), axis=1)\n",
        "X_noisy = X_noisy[np.random.permutation(X.shape[0])]\n",
        "# fait par moi\n",
        "run_svm_cv(X_noisy, y)\n",
        "# fin du fait par moi"
      ],
      "id": "ee530a7f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 6\n"
      ],
      "id": "f6095509"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}